# -*- coding: utf-8 -*-
"""HP-search keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vUYrOIbLc3cypMy2aIcvnpXu5W2SJnzR

### Data prep
"""

!pip install talos kaggle

import tensorflow as tf
import os
import glob
import shutil
import tqdm
import random
from google.colab import files
import os
import numpy as np
random.seed(42)

# upload kaggle credentials file
data = files.upload()

# create directories for storing the kaggle credentials and also for storing the downloaded training data
os.makedirs('/root/.kaggle/', exist_ok=True)
os.makedirs("/content/data", exist_ok=True)

!cp kaggle.json /root/.kaggle/

# download the dataset
!kaggle datasets download -d alxmamaev/flowers-recognition --unzip -p /content/data/

!ls /content/data/flowers/

# function for spliting data for a given class in train-valid-test sets
def split_class(in_path, out_path, class_, train_split=0.7):
    """
    """
    assert 0 < train_split < 1, "The train_split value should be a float between 0 and 1"
    p = os.path.join(in_path, class_)
    ps = glob.glob(os.path.join(p, '*.jpg'))
    random.shuffle(ps)
    size = len(ps)

    val_ratio = (1 - train_split)/2

    p_train = ps[: int(np.ceil(size*train_split))]
    p_val = ps[int(np.ceil(size*train_split)): int(np.ceil(size*train_split)) + int(np.ceil(size*val_ratio))]
    p_test = ps[int(np.ceil(size*train_split)) + int(np.ceil(size*val_ratio)):]

    dest_path = os.path.join(out_path, 'train', str(class_))
    os.makedirs(dest_path, exist_ok=True)

    print("*******[COPYING TRAIN IMAGES]***********")
    for im in tqdm.tqdm(p_train):
        shutil.copy(im, dest_path)

    print("*******[COPYING VALIDATION IMAGES]***********")
    dest_path = os.path.join(out_path, 'valid', str(class_))
    os.makedirs(dest_path, exist_ok=True)
    for im in tqdm.tqdm(p_val):
        shutil.copy(im, dest_path)

    print("*******[COPYING TEST IMAGES]***********")
    dest_path = os.path.join(out_path, 'test', str(class_))
    os.makedirs(dest_path, exist_ok=True)
    for im in tqdm.tqdm(p_test):
        shutil.copy(im, dest_path)

default_p = "/content/data/flowers/"
default_classes = ['daisy', 'dandelion', 'rose', 'tulip', 'sunflower']
out_path = "/content/data/flowers_split"

# function to split the whole dataset into train-valid-test sets

def split_flower_data(root_path=default_p, out_path=out_path, classes=default_classes):
    os.makedirs(out_path, exist_ok=True)

    for cl in classes:
        print("==========CLASS : {}===========".format(cl))
        split_class(root_path, out_path, cl)
        print("===============================\n\n")

"""%%time
split_flower_data()

### Model and training
"""

train_dir = "/content/data/flowers_split/train/"
val_dir = "/content/data/flowers_split/valid/"

# generators
train_img = tf.keras.preprocessing.image.ImageDataGenerator(
        rotation_range=40,
        width_shift_range=0.2,
        height_shift_range=0.2,
        rescale=1./255,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest')

val_img = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)

train_gen = train_img.flow_from_directory(train_dir, target_size=(224, 224), batch_size=32, shuffle=True)
val_gen = val_img.flow_from_directory(val_dir, target_size=(224, 224), batch_size=256, shuffle=False)

tf.keras.backend.clear_session()
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Conv2D(64, (5, 5), padding='same', activation='elu', input_shape=(224, 224, 3)))
model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))
model.add(tf.keras.layers.Dropout(0.25))
model.add(tf.keras.layers.BatchNormalization())

model.add(tf.keras.layers.Conv2D(128, (5, 5), padding='same', activation='elu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))
model.add(tf.keras.layers.Dropout(0.25))
model.add(tf.keras.layers.BatchNormalization())

model.add(tf.keras.layers.Conv2D(256, (5, 5), padding='same', activation='elu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))
model.add(tf.keras.layers.Dropout(0.25))
model.add(tf.keras.layers.GlobalAveragePooling2D())
model.add(tf.keras.layers.BatchNormalization())

model.add(tf.keras.layers.Dense(256, activation='elu'))
model.add(tf.keras.layers.Dropout(0.1))
model.add(tf.keras.layers.Dense(5, activation='softmax'))


tpu_model = tf.contrib.tpu.keras_to_tpu_model(
        model,
        strategy=tf.contrib.tpu.TPUDistributionStrategy(
            tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
        )
    )

tpu_model.compile(
        optimizer=tf.train.AdamOptimizer(learning_rate=1e-3),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

tpu_model.summary()

"""%%time

history = tpu_model.fit_generator(train_gen, epochs=100, validation_data=val_gen, use_multiprocessing=True, shuffle=False, workers=5, max_queue_size=16)
"""



# Step 3: conver the model to tpu model and compile with tensorflow optimizer.
    tpu_model = tf.contrib.tpu.keras_to_tpu_model(
        model,
        strategy=tf.contrib.tpu.TPUDistributionStrategy(
            tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
        )
    )
    tpu_model.compile(
        optimizer=tf.train.AdamOptimizer(learning_rate=1e-3, ),
        loss=tf.keras.losses.categorical_crossentropy,
        metrics=['categorical_accuracy']
    )

    # Step 4: Train the model on TPU with fixed batch size.
    out = tpu_model.fit(
        x, y, epochs=10, batch_size = 1024,
        verbose=0,
        validation_data=[x_val, y_val]
    )
    # Step 5: Return the history output and synced back cpu model.
    return out, tpu_model.sync_to_cpu()